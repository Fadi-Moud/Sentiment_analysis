{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FbCgk1zx9bim"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLKkisttKixA",
        "outputId": "a2895b14-7194-4600-e2c5-60a373ef35ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting tabulate\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: tabulate\n",
            "Successfully installed tabulate-0.9.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: typing-extensions in /home/fadilath/.local/lib/python3.10/site-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: jinja2 in /home/fadilath/.local/lib/python3.10/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /home/fadilath/.local/lib/python3.10/site-packages (from torch) (3.12.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
            "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5.post0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.26.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/fadilath/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Pf7p5T-mTG",
        "outputId": "baa189e1-5de0-4363-fb42-4a14cc3f9fed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.config', 'dataset_final_all.csv', 'sample_data']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "O1TX0mza_MPI",
        "outputId": "aba53269-215b-47ef-cdb2-94be850dbec6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_title</th>\n",
              "      <th>niveau</th>\n",
              "      <th>language</th>\n",
              "      <th>target</th>\n",
              "      <th>review_all</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11995</th>\n",
              "      <td>60925</td>\n",
              "      <td>2</td>\n",
              "      <td>I am disappointed with the Kona Grill Mat. It ...</td>\n",
              "      <td>Best Grill Mat???</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>-1</td>\n",
              "      <td>Best Grill Mat??? I am disappointed with the K...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11996</th>\n",
              "      <td>45700</td>\n",
              "      <td>2</td>\n",
              "      <td>Lasted two months before strap holder broke - ...</td>\n",
              "      <td>Lasted Two Months</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>-1</td>\n",
              "      <td>Lasted Two Months Lasted two months before str...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11997</th>\n",
              "      <td>69161</td>\n",
              "      <td>2</td>\n",
              "      <td>Disappointed in these. The tips don't line up,...</td>\n",
              "      <td>Poor quality</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>-1</td>\n",
              "      <td>Poor quality Disappointed in these. The tips d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11998</th>\n",
              "      <td>71355</td>\n",
              "      <td>2</td>\n",
              "      <td>Battery charger lasted a day Poor construction</td>\n",
              "      <td>Two Stars</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>-1</td>\n",
              "      <td>Two Stars Battery charger lasted a day Poor co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11999</th>\n",
              "      <td>59534</td>\n",
              "      <td>2</td>\n",
              "      <td>Product worked for 4 months then stopped. Look...</td>\n",
              "      <td>Product did not work</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>-1</td>\n",
              "      <td>Product did not work Product worked for 4 mont...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  stars                                        review_body  \\\n",
              "11995       60925      2  I am disappointed with the Kona Grill Mat. It ...   \n",
              "11996       45700      2  Lasted two months before strap holder broke - ...   \n",
              "11997       69161      2  Disappointed in these. The tips don't line up,...   \n",
              "11998       71355      2     Battery charger lasted a day Poor construction   \n",
              "11999       59534      2  Product worked for 4 months then stopped. Look...   \n",
              "\n",
              "               review_title       niveau language  target  \\\n",
              "11995     Best Grill Mat???  insatisfait       en      -1   \n",
              "11996     Lasted Two Months  insatisfait       en      -1   \n",
              "11997          Poor quality  insatisfait       en      -1   \n",
              "11998             Two Stars  insatisfait       en      -1   \n",
              "11999  Product did not work  insatisfait       en      -1   \n",
              "\n",
              "                                              review_all  \n",
              "11995  Best Grill Mat??? I am disappointed with the K...  \n",
              "11996  Lasted Two Months Lasted two months before str...  \n",
              "11997  Poor quality Disappointed in these. The tips d...  \n",
              "11998  Two Stars Battery charger lasted a day Poor co...  \n",
              "11999  Product did not work Product worked for 4 mont...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt=pd.read_csv(\"dataset_final_all.csv\")\n",
        "dt.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0PguSxrleGW_"
      },
      "outputs": [],
      "source": [
        "def assign_label(val):\n",
        "  if val=='satisfait':\n",
        "    return 2\n",
        "  elif val=='mitige':\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "DyB6gu3jeb1Q",
        "outputId": "337b0c0a-ed1e-4577-9856-72767517e18a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_title</th>\n",
              "      <th>niveau</th>\n",
              "      <th>language</th>\n",
              "      <th>target</th>\n",
              "      <th>review_all</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>143119</td>\n",
              "      <td>4</td>\n",
              "      <td>La house est conforme à l'offre. Elle à été bi...</td>\n",
              "      <td>Bon produit</td>\n",
              "      <td>satisfait</td>\n",
              "      <td>fr</td>\n",
              "      <td>2</td>\n",
              "      <td>Bon produit La house est conforme à l'offre. E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>145657</td>\n",
              "      <td>4</td>\n",
              "      <td>Pour faire court le produit était mal fermé ma...</td>\n",
              "      <td>Très bon shampooing pour enlever les reflets j...</td>\n",
              "      <td>satisfait</td>\n",
              "      <td>fr</td>\n",
              "      <td>2</td>\n",
              "      <td>Très bon shampooing pour enlever les reflets j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>157590</td>\n",
              "      <td>4</td>\n",
              "      <td>bon produit qualité = au prix</td>\n",
              "      <td>utilisation facile</td>\n",
              "      <td>satisfait</td>\n",
              "      <td>fr</td>\n",
              "      <td>2</td>\n",
              "      <td>utilisation facile bon produit qualité = au prix</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151657</td>\n",
              "      <td>4</td>\n",
              "      <td>Super rapport qualité/prix. Ce sont des pièces...</td>\n",
              "      <td>Satisfait</td>\n",
              "      <td>satisfait</td>\n",
              "      <td>fr</td>\n",
              "      <td>2</td>\n",
              "      <td>Satisfait Super rapport qualité/prix. Ce sont ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>124029</td>\n",
              "      <td>4</td>\n",
              "      <td>Le colis a mis le temps pour venir, mais il es...</td>\n",
              "      <td>Maillot de sport 2 étoiles</td>\n",
              "      <td>satisfait</td>\n",
              "      <td>fr</td>\n",
              "      <td>2</td>\n",
              "      <td>Maillot de sport 2 étoiles Le colis a mis le t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11995</th>\n",
              "      <td>60925</td>\n",
              "      <td>2</td>\n",
              "      <td>I am disappointed with the Kona Grill Mat. It ...</td>\n",
              "      <td>Best Grill Mat???</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>Best Grill Mat??? I am disappointed with the K...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11996</th>\n",
              "      <td>45700</td>\n",
              "      <td>2</td>\n",
              "      <td>Lasted two months before strap holder broke - ...</td>\n",
              "      <td>Lasted Two Months</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>Lasted Two Months Lasted two months before str...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11997</th>\n",
              "      <td>69161</td>\n",
              "      <td>2</td>\n",
              "      <td>Disappointed in these. The tips don't line up,...</td>\n",
              "      <td>Poor quality</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>Poor quality Disappointed in these. The tips d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11998</th>\n",
              "      <td>71355</td>\n",
              "      <td>2</td>\n",
              "      <td>Battery charger lasted a day Poor construction</td>\n",
              "      <td>Two Stars</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>Two Stars Battery charger lasted a day Poor co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11999</th>\n",
              "      <td>59534</td>\n",
              "      <td>2</td>\n",
              "      <td>Product worked for 4 months then stopped. Look...</td>\n",
              "      <td>Product did not work</td>\n",
              "      <td>insatisfait</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>Product did not work Product worked for 4 mont...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  stars                                        review_body  \\\n",
              "0          143119      4  La house est conforme à l'offre. Elle à été bi...   \n",
              "1          145657      4  Pour faire court le produit était mal fermé ma...   \n",
              "2          157590      4                      bon produit qualité = au prix   \n",
              "3          151657      4  Super rapport qualité/prix. Ce sont des pièces...   \n",
              "4          124029      4  Le colis a mis le temps pour venir, mais il es...   \n",
              "...           ...    ...                                                ...   \n",
              "11995       60925      2  I am disappointed with the Kona Grill Mat. It ...   \n",
              "11996       45700      2  Lasted two months before strap holder broke - ...   \n",
              "11997       69161      2  Disappointed in these. The tips don't line up,...   \n",
              "11998       71355      2     Battery charger lasted a day Poor construction   \n",
              "11999       59534      2  Product worked for 4 months then stopped. Look...   \n",
              "\n",
              "                                            review_title       niveau  \\\n",
              "0                                            Bon produit    satisfait   \n",
              "1      Très bon shampooing pour enlever les reflets j...    satisfait   \n",
              "2                                     utilisation facile    satisfait   \n",
              "3                                              Satisfait    satisfait   \n",
              "4                             Maillot de sport 2 étoiles    satisfait   \n",
              "...                                                  ...          ...   \n",
              "11995                                  Best Grill Mat???  insatisfait   \n",
              "11996                                  Lasted Two Months  insatisfait   \n",
              "11997                                       Poor quality  insatisfait   \n",
              "11998                                          Two Stars  insatisfait   \n",
              "11999                               Product did not work  insatisfait   \n",
              "\n",
              "      language  target                                         review_all  \n",
              "0           fr       2  Bon produit La house est conforme à l'offre. E...  \n",
              "1           fr       2  Très bon shampooing pour enlever les reflets j...  \n",
              "2           fr       2   utilisation facile bon produit qualité = au prix  \n",
              "3           fr       2  Satisfait Super rapport qualité/prix. Ce sont ...  \n",
              "4           fr       2  Maillot de sport 2 étoiles Le colis a mis le t...  \n",
              "...        ...     ...                                                ...  \n",
              "11995       en       0  Best Grill Mat??? I am disappointed with the K...  \n",
              "11996       en       0  Lasted Two Months Lasted two months before str...  \n",
              "11997       en       0  Poor quality Disappointed in these. The tips d...  \n",
              "11998       en       0  Two Stars Battery charger lasted a day Poor co...  \n",
              "11999       en       0  Product did not work Product worked for 4 mont...  \n",
              "\n",
              "[12000 rows x 8 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt['target']=dt['niveau'].apply(lambda x:assign_label(x))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DdtuQQmNAm7j"
      },
      "outputs": [],
      "source": [
        "text = dt.review_all.values \n",
        "label = dt.target.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6ysNHSW1_eBP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 996k/996k [00:01<00:00, 690kB/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 79.0kB/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 625/625 [00:00<00:00, 1.97MB/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained( \n",
        "    'bert-base-multilingual-cased', \n",
        "    do_lower_case = True \n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E3nnfR1BGT8",
        "outputId": "17cd2151-22cc-45fb-a114-1308fe356542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒══════════╤═════════════╕\n",
            "│ Tokens   │   Token IDs │\n",
            "╞══════════╪═════════════╡\n",
            "│ coli     │       77019 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##s      │       10107 │\n",
            "├──────────┼─────────────┤\n",
            "│ vol      │       12714 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##e      │       10112 │\n",
            "├──────────┼─────────────┤\n",
            "│ le       │       10141 │\n",
            "├──────────┼─────────────┤\n",
            "│ vende    │       64298 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##ur     │       10546 │\n",
            "├──────────┼─────────────┤\n",
            "│ a        │         169 │\n",
            "├──────────┼─────────────┤\n",
            "│ de       │       10104 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##pose   │       50864 │\n",
            "├──────────┼─────────────┤\n",
            "│ le       │       10141 │\n",
            "├──────────┼─────────────┤\n",
            "│ coli     │       77019 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##s      │       10107 │\n",
            "├──────────┼─────────────┤\n",
            "│ dans     │       10260 │\n",
            "├──────────┼─────────────┤\n",
            "│ la       │       10109 │\n",
            "├──────────┼─────────────┤\n",
            "│ bo       │       20506 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##ite    │       12704 │\n",
            "├──────────┼─────────────┤\n",
            "│ mais     │       10614 │\n",
            "├──────────┼─────────────┤\n",
            "│ je       │       10144 │\n",
            "├──────────┼─────────────┤\n",
            "│ ne       │       10554 │\n",
            "├──────────┼─────────────┤\n",
            "│ l        │         180 │\n",
            "├──────────┼─────────────┤\n",
            "│ '        │         112 │\n",
            "├──────────┼─────────────┤\n",
            "│ ai       │       11346 │\n",
            "├──────────┼─────────────┤\n",
            "│ jamais   │       22168 │\n",
            "├──────────┼─────────────┤\n",
            "│ trouve   │       15969 │\n",
            "├──────────┼─────────────┤\n",
            "│ ,        │         117 │\n",
            "├──────────┼─────────────┤\n",
            "│ tout     │       13003 │\n",
            "├──────────┼─────────────┤\n",
            "│ ce       │       10794 │\n",
            "├──────────┼─────────────┤\n",
            "│ qu       │       10608 │\n",
            "├──────────┼─────────────┤\n",
            "│ '        │         112 │\n",
            "├──────────┼─────────────┤\n",
            "│ il       │       10154 │\n",
            "├──────────┼─────────────┤\n",
            "│ propose  │       30027 │\n",
            "├──────────┼─────────────┤\n",
            "│ c        │         171 │\n",
            "├──────────┼─────────────┤\n",
            "│ '        │         112 │\n",
            "├──────────┼─────────────┤\n",
            "│ est      │       10176 │\n",
            "├──────────┼─────────────┤\n",
            "│ de       │       10104 │\n",
            "├──────────┼─────────────┤\n",
            "│ me       │       10911 │\n",
            "├──────────┼─────────────┤\n",
            "│ faire    │       14131 │\n",
            "├──────────┼─────────────┤\n",
            "│ une      │       10231 │\n",
            "├──────────┼─────────────┤\n",
            "│ att      │       10788 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##esta   │       18487 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##tion   │       10822 │\n",
            "├──────────┼─────────────┤\n",
            "│ de       │       10104 │\n",
            "├──────────┼─────────────┤\n",
            "│ perte    │       72220 │\n",
            "├──────────┼─────────────┤\n",
            "│ de       │       10104 │\n",
            "├──────────┼─────────────┤\n",
            "│ coli     │       77019 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##s      │       10107 │\n",
            "├──────────┼─────────────┤\n",
            "│ ,        │         117 │\n",
            "├──────────┼─────────────┤\n",
            "│ ,        │         117 │\n",
            "├──────────┼─────────────┤\n",
            "│ ,        │         117 │\n",
            "├──────────┼─────────────┤\n",
            "│ ,        │         117 │\n",
            "├──────────┼─────────────┤\n",
            "│ pour     │       10322 │\n",
            "├──────────┼─────────────┤\n",
            "│ qui      │       10355 │\n",
            "├──────────┼─────────────┤\n",
            "│ ?        │         136 │\n",
            "├──────────┼─────────────┤\n",
            "│ pour     │       10322 │\n",
            "├──────────┼─────────────┤\n",
            "│ quoi     │       84359 │\n",
            "├──────────┼─────────────┤\n",
            "│ faire    │       14131 │\n",
            "├──────────┼─────────────┤\n",
            "│ ?        │         136 │\n",
            "├──────────┼─────────────┤\n",
            "│ ri       │       29956 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##dic    │       55170 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##ule    │       16115 │\n",
            "╘══════════╧═════════════╛\n"
          ]
        }
      ],
      "source": [
        "def print_rand_sentence():\n",
        "  '''Displays the tokens and respective IDs of a random text sample'''\n",
        "  index = random.randint(0, len(text)-1)\n",
        "  table = np.array([tokenizer.tokenize(text[index]), \n",
        "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
        "  print(tabulate(table,\n",
        "                 headers = ['Tokens', 'Token IDs'],\n",
        "                 tablefmt = 'fancy_grid'))\n",
        "\n",
        "print_rand_sentence()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DP-CXkmVBe_M"
      },
      "outputs": [],
      "source": [
        "\n",
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 24,\n",
        "                        truncation=True,\n",
        "                        padding= 'max_length',\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "\n",
        "for sample in text:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "label = torch.tensor(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhqS3ANVC6e1",
        "outputId": "0262e1bf-815e-4691-d045-6fc7ad93ccc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  101, 67695, 29264, 11681, 67695,   106, 22242, 12150, 10446, 65937,\n",
              "        10112,   117, 10144,   180, 72894, 34508, 13477, 10152, 17916,   119,\n",
              "        20445, 19685, 11481,   102])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_id[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX7B-6SCC-fu",
        "outputId": "4c7e0a24-d4d6-43ab-83f5-088a82d9e2d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒══════════╤═════════════╤══════════════════╕\n",
            "│ Tokens   │   Token IDs │   Attention Mask │\n",
            "╞══════════╪═════════════╪══════════════════╡\n",
            "│ [CLS]    │         101 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ bien     │       12028 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ mais     │       10614 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ .        │         119 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ .        │         119 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ .        │         119 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ aim      │       56048 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ ##ant    │       11236 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ un       │       10119 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ peu      │       14574 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ faible   │       37724 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ mais     │       10614 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ sino     │       18874 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ ##n      │       10115 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ ,        │         117 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ fait     │       11329 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ son      │       10312 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ job      │       23627 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ .        │         119 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ .        │         119 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ .        │         119 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ dom      │       31858 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ ##mage   │       65461 │                1 │\n",
            "├──────────┼─────────────┼──────────────────┤\n",
            "│ [SEP]    │         102 │                1 │\n",
            "╘══════════╧═════════════╧══════════════════╛\n"
          ]
        }
      ],
      "source": [
        "def print_rand_sentence_encoding():\n",
        "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
        "  index = random.randint(0, len(text) - 1)\n",
        "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
        "  token_ids = [i.numpy() for i in token_id[index]]\n",
        "  attention = [i.numpy() for i in attention_masks[index]]\n",
        "\n",
        "  table = np.array([tokens, token_ids, attention]).T\n",
        "  print(tabulate(table, \n",
        "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
        "                 tablefmt = 'fancy_grid'))\n",
        "\n",
        "print_rand_sentence_encoding()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30Vjdr_wKH9m"
      },
      "source": [
        "## Fractionnement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BYZZhYG7J9Sb"
      },
      "outputs": [],
      "source": [
        "\n",
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 8\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(label)),\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = label)\n",
        "\n",
        "# Train and validation sets\n",
        "train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          label[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        label[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wMe9gRQhLL_-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def b_tp(preds, label):\n",
        "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "  return sum([preds == label and preds == 1 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_fp(preds, label):\n",
        "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "  return sum([preds != label and preds == 1 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_tn(preds, label):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == label and preds == 0 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_fn(preds, label):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != label and preds == 0 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_kn(preds, label):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == label and preds == -1 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_kp(preds, label):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != label and preds == -1 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_metrics(preds, label):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - specificity = TN / (TN + FP)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, label)\n",
        "  tn = b_tn(preds, label)\n",
        "  fp = b_fp(preds, label)\n",
        "  fn = b_fn(preds, label)\n",
        "  b_accuracy = (tp + tn) / len(label)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
        "  return b_accuracy, b_precision, b_recall, b_specificity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U3dwv2Bnb1Kj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def b_tp(preds, label):\n",
        "    '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "    return sum([preds == label and preds == 1 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_fp(preds, label):\n",
        "    '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "    return sum([preds != label and preds == 1 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_tn(preds, label):\n",
        "    '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "    return sum([preds == label and preds == 0 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_fn(preds, label):\n",
        "    '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "    return sum([preds != label and preds == 0 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_kn(preds, label):\n",
        "    '''Returns True Negatives (TN): count of correct predictions of actual class -1'''\n",
        "    return sum([preds == label and preds == -1 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_kp(preds, label):\n",
        "    '''Returns False Negatives (FN): count of wrong predictions of actual class -1'''\n",
        "    return sum([preds != label and preds == -1 for preds, label in zip(preds, label)])\n",
        "\n",
        "def b_metrics(preds, label):\n",
        "    '''\n",
        "    Returns the following metrics:\n",
        "        - accuracy    = (TP + TN) / N\n",
        "        - precision   = TP / (TP + FP)\n",
        "        - recall      = TP / (TP + FN)\n",
        "        - specificity = TN / (TN + FP)\n",
        "    '''\n",
        "    preds = np.argmax(preds, axis=1).flatten()\n",
        "    label = label.flatten()\n",
        "    tp = b_tp(preds, label)\n",
        "    tn = b_tn(preds, label)\n",
        "    fp = b_fp(preds, label)\n",
        "    fn = b_fn(preds, label)\n",
        "    kn = b_kn(preds, label)\n",
        "    kp = b_kp(preds, label)\n",
        "    b_accuracy = (tp + tn + kn) / len(label)\n",
        "    b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "    b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "    b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
        "    return b_accuracy, b_precision, b_recall, b_specificity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ0Zkl9pjEt0",
        "outputId": "dac903f6-b865-4a01-aea2-4d72ae841d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX9GYmZiroaP"
      },
      "outputs": [],
      "source": [
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yOnn7ms41IK"
      },
      "outputs": [],
      "source": [
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "xTz-k_3cPvco",
        "outputId": "107eee43-5137-4549-a221-4262dd911a1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), \n\u001b[1;32m     11\u001b[0m                               lr \u001b[39m=\u001b[39m \u001b[39m5e-5\u001b[39m,\n\u001b[1;32m     12\u001b[0m                               eps \u001b[39m=\u001b[39m \u001b[39m1e-08\u001b[39m\n\u001b[1;32m     13\u001b[0m                               )\n\u001b[1;32m     15\u001b[0m \u001b[39m# Run on GPU\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model\u001b[39m.\u001b[39;49mcuda()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-multilingual-cased',\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 5e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "JAKYXlEfZae2",
        "outputId": "263e2deb-baca-499f-b545-c40cb5a69f57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-48d2577f0170>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-48d2577f0170>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 2\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
